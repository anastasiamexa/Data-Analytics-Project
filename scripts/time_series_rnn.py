# -*- coding: utf-8 -*-
"""time series RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i6E9d8MnYl38W9SFd2YVdw9foKvnFRrK
"""

import pandas as pd
from math import sqrt
import numpy as np
from itertools import chain
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import SimpleRNN, LSTM, GRU
import matplotlib.pyplot as plt
from numpy import concatenate
from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_squared_error, r2_score

def timeseries_to_supervised(df, n_in, n_out):
  agg = pd.DataFrame()
  for i in range(n_in, 0, -1):
    df_shifted = df.shift(i).copy()
    df_shifted.rename(columns=lambda x: ('%s(t-%d)' % (x, i)), inplace=True)
    agg = pd.concat([agg, df_shifted], axis=1)
  for i in range(0, n_out):
    df_shifted = df.shift(-i).copy()
    if i == 0:
      df_shifted.rename(columns=lambda x: ('%s(t)' % (x)), inplace=True)
    else:
      df_shifted.rename(columns=lambda x: ('%s(t+%d)' % (x, i)), inplace=True)
    agg = pd.concat([agg, df_shifted], axis=1)
  agg.dropna(inplace=True)
  return agg

X = pd.read_excel('data_akbilgic(1).xlsx',usecols = "B:I")  # loading the data

# normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(X)

# convert ndarray to pandas dataframe
scaled = pd.DataFrame(scaled, columns=["data.1", "data.2", "data.3", "data.4", "data.5", "data.6", "data.7", "data.8"])

n_steps = 6
sdf = timeseries_to_supervised(scaled, n_steps, 1)
lst = []
for i in range(1,8):
  lst.append((n_steps+1)*8 - i)
# removing the columns we do not need (SP(t), DAX(t), FTSE(t), NIKKEI(t), BOVESPA(t), EU(t), EM(t))
sdf.drop(sdf.columns[lst], axis=1, inplace=True)
print(sdf)
#sdf.to_excel("test.xlsx")

sdf = sdf.values

# Split data into train and test
len_data = sdf.shape[0]
print("Total length of data:", len_data)
train_size = int(len_data * .7)
print ("Train size: %d" % train_size)
print ("Test size: %d" % (len_data - train_size))

train = sdf[:train_size, :]
test = sdf[train_size:, :]

# split into input and outputs
train_X, train_y = train[:, :-1], train[:, -1]
test_X, test_y = test[:, :-1], test[:, -1]

# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))

train_y = np.reshape(train_y, (train_size, 1, 1))
test_y = np.reshape(test_y, (len_data - train_size, 1, 1))

print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)

# Building a model with SimpleRNN
batch_size = 1 #1
model = Sequential()

model.add(SimpleRNN(units=300, input_shape=(train_X.shape[1], train_X.shape[2]), activation="relu", return_sequences=True))
model.add(Dense(100, activation="relu")) 
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

history = model.fit(train_X,train_y, epochs=50, batch_size=batch_size, verbose=2)

# plot history
plt.plot(history.history['loss'], label='loss')
plt.legend()
plt.show()

# make a prediction
testPredict = model.predict(test_X)
test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))

# invert scaling for prediction
testPredict = np.reshape(testPredict, (testPredict.shape[0], 1))
inv_testPredict = concatenate((testPredict, test_X[:, -7:]), axis=1)
inv_testPredict = scaler.inverse_transform(inv_testPredict)
inv_testPredict = inv_testPredict[:,0]

# invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
test_y = concatenate((test_y, test_X[:, -7:]), axis=1)
test_y = scaler.inverse_transform(test_y)
test_y = test_y[:,0]

# calculate error
print("Test MSE: ", mean_squared_error(test_y, inv_testPredict))
print("Test MAE: ", sum(abs(test_y-inv_testPredict))/test_y.shape[0])
print("Test R2: ", r2_score(test_y, inv_testPredict))

trainPredict = model.predict(train_X)
train_X = train_X.reshape((train_X.shape[0], train_X.shape[2]))

# invert scaling for prediction
trainPredict = np.reshape(trainPredict, (trainPredict.shape[0], 1))
inv_trainPredict = concatenate((trainPredict, train_X[:, -7:]), axis=1)
inv_trainPredict = scaler.inverse_transform(inv_trainPredict)
inv_trainPredict = inv_trainPredict[:,0]

# invert scaling for actual
train_y = train_y.reshape((len(train_y), 1))
train_y = concatenate((train_y, train_X[:, -7:]), axis=1)
train_y = scaler.inverse_transform(train_y)
train_y = train_y[:,0]

# Finally, we check the result in a plot. 
# A vertical line in a plot identifies a splitting point between 
# the training and the test part.
predicted = np.concatenate((inv_trainPredict,inv_testPredict),axis=0)

original = np.concatenate((train_y,test_y),axis=0)
predicted = np.concatenate((inv_trainPredict,inv_testPredict),axis=0)
index = range(0, original.shape[0])
plt.plot(index,original, 'g')
plt.plot(index,predicted, 'r')
plt.axvline(scaled.index[train_size], c="b")
plt.show()